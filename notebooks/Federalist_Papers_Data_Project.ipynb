{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Attempt-to-Fix-SSL-Issue\" data-toc-modified-id=\"Attempt-to-Fix-SSL-Issue-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Attempt to Fix SSL Issue</a></div><div class=\"lev1 toc-item\"><a href=\"#Project-Overview\" data-toc-modified-id=\"Project-Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Project Overview</a></div><div class=\"lev1 toc-item\"><a href=\"#Part-1---Scraping-The-Federalist-Papers\" data-toc-modified-id=\"Part-1---Scraping-The-Federalist-Papers-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Part 1 - Scraping The Federalist Papers</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Attempt to Fix SSL Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Received the following error when trying to scrap the federalist papers using the Python 3.8 conda environment:\n",
    "\n",
    "```\n",
    "SSLError: HTTPSConnectionPool(host='avalon.law.yale.edu', port=443): Max retries exceeded with url: /18th_century/fed01.asp (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))\n",
    "```\n",
    "\n",
    "Possibly [this SO answer](https://stackoverflow.com/a/45909353) is the reason why. I tried [this SO answer](https://stackoverflow.com/a/55632553) by adding to the shell `PATH` variable (did not modify system `PATH`), but no dice. I also tried `verify=False` in the `get` method but still have the same error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Shell command to find out where anaconda is installed\n",
    "anaconda_exec = !where anaconda\n",
    "anaconda_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Modify path \n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\")\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\\\\Scripts\")\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\\\\Library\\\\bin\")\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\\\\envs\\\\scipybase_Jun2021\")\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\\\\envs\\\\scipybase_Jun2021\\\\Scripts\")\n",
    "sys.path.append(\"C:\\\\Users\\\\Sonya\\\\Anaconda3\\\\envs\\\\scipybase_Jun2021\\\\Library\\\\bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify path env variable\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "lorem ipsum dolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1 - Scraping The Federalist Papers ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This section pulls the federalist papers from a website to give an example of the jupyter `%run` command - you don't need to execute these cells if you have already retrieved the federalist papers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to retrieve the contents of all the federalist papers from the law school website. We can do this using the `requests` package to retrieve the HTML for each paper's web page, then use the `bs4` package to parse that HTML, and finally save the actual text contents of each paper to our data folder. We have a stand-alone python script at `scripts/scrape_federalist_papers.py` which does this; as an exercise you can try writing such a script yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter provides the `%run` magic command for executing code in external python scripts, where all the script variables become available within your notebook namespace. We'll run our scraping script now and write the federalist papers as individual text files in the `data/federalist_papers` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Box Sync\\Projects\\workshop-yale-som-python\\scripts\\scrape_federalist_papers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtesting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "%run ../scripts/scrape_federalist_papers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A list variable called `authors` that stores the paper authors is created in the external script. \n",
    "# After using the `%run` magic we now have this list variable available in our notebook as well:\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Part 2 - Mimicking the three Federalist authors ###\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def draw_word(distrn):\n",
    "    words = list(distrn)\n",
    "    freqs = [freq for w, freq in distrn.items()]\n",
    "    total = sum(freqs)\n",
    "    probs = [freq/total for freq in freqs]\n",
    "    return numpy.random.choice(words, p=probs)\n",
    "\n",
    "def generate_with_trigrams(text, word=None, num=100):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    trigrams = nltk.trigrams(tokens)\n",
    "    condition_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams)\n",
    "    cfdist = nltk.ConditionalFreqDist(condition_pairs)\n",
    "    if word is None:\n",
    "        prev = draw_word(nltk.FreqDist(tokens))\n",
    "        word = draw_word(nltk.ConditionalFreqDist(nltk.bigrams(tokens))[prev])\n",
    "    elif len(word.split()) == 1:\n",
    "        prev = word\n",
    "        word = draw_word(nltk.ConditionalFreqDist(nltk.bigrams(tokens))[prev])\n",
    "        # will give an error if this pair doesn't show up in the text\n",
    "    else:\n",
    "        prev, word = word.split()[:2]\n",
    "    print(prev, end=' ')\n",
    "    for i in range(1, num):\n",
    "        print(word, end=' ')\n",
    "        prev, word = word, draw_word(cfdist[(prev, word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path  # This is a useful class for constructing file system paths\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the data folder and the number of papers to scrape (papers are identified by integers)\n",
    "datapath = Path(r\"../data/federalist_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author = pd.read_csv(datapath / \"authors.csv\", squeeze=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-060e9c7d36a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Hamilton\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mhamilton\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Madison\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# each author will have all his papers merged into a single string\n",
    "hamilton = \"\"\n",
    "madison = \"\"\n",
    "jay = \"\"\n",
    "\n",
    "docnames = [datapath / f for f in os.listdir(datapath) if f[-4:]==\".txt\"]\n",
    "docnames.sort()\n",
    "\n",
    "N = len(docnames)\n",
    "for i in range(N):\n",
    "    with open(docnames[i], 'r') as f:\n",
    "        if author[i] == \"Hamilton\":\n",
    "            hamilton += f.read() + \" \"\n",
    "        elif author[i] == \"Madison\":\n",
    "            madison += f.read() + \" \"\n",
    "        elif author[i] == \"Jay\":\n",
    "            jay += f.read() + \" \"\n",
    "        else:\n",
    "            # discard papers with mixed authorship e.g. \"Hamilton and Madison\"\n",
    "            pass\n",
    "\n",
    "len(hamilton)\n",
    "len(madison)\n",
    "len(jay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generate_with_trigrams(hamilton, \"The\")\n",
    "# generate_with_trigrams(madison, \"The\")\n",
    "# generate_with_trigrams(jay, \"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1\n"
     ]
    }
   ],
   "source": [
    "%env CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Part 3 - Creating data frame of token frequencies ###\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = len(docnames)\n",
    "tables = [None]*N\n",
    "for i in range(N):\n",
    "    with open(docnames[i], 'r') as f:\n",
    "        doc = f.read()\n",
    "        doc = doc.replace(\"To the People of the State of New York:\", \"\")\n",
    "        doc = doc.replace(\"PUBLIUS\", \"\")\n",
    "        doc = doc.replace(\"Ã¥\", \"\")\n",
    "        doc = re.sub(\"[0-9]+\", \"\", doc)\n",
    "        doc = doc.lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(doc)\n",
    "        tables[i] = nltk.FreqDist(tokens)\n",
    "\n",
    "df = pd.DataFrame(tables)\n",
    "\n",
    "# fill in zeros\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# divide rows by totals\n",
    "for i in range(N):\n",
    "    s = sum(df.iloc[i])\n",
    "    df.iloc[i] = [n/s for n in df.iloc[i]]\n",
    "\n",
    "df.iloc[0] # check a row to make sure it worked\n",
    "\n",
    "# write as csv\n",
    "df.to_csv(\"../federalist.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# write authors as well\n",
    "with open(\"../authors.csv\", \"w\") as f:\n",
    "    f.write(\"author\\n\")\n",
    "    for a in author:\n",
    "        f.write(a + \"\\n\")\n",
    "\n",
    "# and write tokens\n",
    "with open(\"../tokens.csv\", \"w\") as f:\n",
    "    f.write(\"token\\n\")\n",
    "    for t in list(df):\n",
    "        f.write('\"' + t + '\"\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:scipybase_Apr2019]",
   "language": "python",
   "name": "conda-env-scipybase_Apr2019-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
